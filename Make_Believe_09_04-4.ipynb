{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Baseline classification models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a model to then finetune, first I am going to compare the baseline sklearn models trained on the dataset with different feature sets. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Prefiltering -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "from json import JSONEncoder\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, numpy.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the process a little bit I will create some functions to fit different models according to different features and output a dictionary of results we can analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first I will define a couple of functions to speed up the process and make it reusable\n",
    "\n",
    "def data_preprocess(data):\n",
    "  \n",
    "  '''We input the data with the linguistic features\n",
    "  and it returns the data with the polarity columns in 0 for fake and 1 for true\n",
    "  as well as the outcomes\n",
    "  '''\n",
    "\n",
    "  if data[\"Polarity\"][0] != 0 or 1:\n",
    "    data[\"Polarity\"] = data[\"Polarity\"].replace({\"True\":1, \"Fake\":0, \"TRUE\":1})\n",
    "    \n",
    "  outcomes = [\"Fake\",\"Real\"]\n",
    "  \n",
    "  return data, outcomes\n",
    "\n",
    "def test_train(data, features):\n",
    "\n",
    "  '''We input the training data and a list of the features we want to train the models with.\n",
    "  it returns the data split in test/train\n",
    "  '''\n",
    "\n",
    "  feature_cols = features\n",
    "  # data = data[feature_cols + [\"Polarity\"]].dropna()\n",
    "\n",
    "  X = data[feature_cols]\n",
    "  y = data[\"Polarity\"] #outcomes 0 or 1\n",
    "\n",
    "\n",
    "  print(\"Info: {} features were passed at the fit step\\n:\".format(X.shape[1]))\n",
    "  for feature in feature_cols:\n",
    "    print(feature)\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, shuffle=True, random_state=16) #stratify so that the proportion of the train data is the same for fake and true\n",
    "\n",
    "  return X, y, X_train, X_test, y_train, y_test, features\n",
    "\n",
    "def class_report(y_test, y_pred):\n",
    "\n",
    "  outcomes = [\"Fake\",\"Real\"]\n",
    "\n",
    "  scores = classification_report(y_test, y_pred, target_names=outcomes)\n",
    "\n",
    "  print(scores)\n",
    "\n",
    "  return scores\n",
    "\n",
    "def compare_models_cross_val(model, X, y):\n",
    "  \n",
    "  '''for each model, it calculates the cross validation accuracy for k-n\n",
    "  as well as the mean cross validation score on the given data\n",
    "\n",
    "  '''\n",
    "\n",
    "  print(\"CROSS VALIDATION\\n\")\n",
    "\n",
    "  model_name = str(model)[:-2]\n",
    "\n",
    "  if model_name == \"SVC(kernel=\\'linear\":\n",
    "\n",
    "    model_name = \"SVC\"\n",
    "\n",
    "\n",
    "  # for model in models:\n",
    "  kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "  cv_score = cross_val_score(model, X, y, cv = kfold, scoring=\"f1\") # we have to change the metric to the one we want to measure the models by\n",
    "  print(\"Cross validation f1 {} for each of the 5 iterations:\".format(model_name))\n",
    "  for score in cv_score:\n",
    "    print(round(score*100, 2), \"%\")\n",
    "\n",
    "  mean_f1 = np.mean(cv_score)\n",
    "\n",
    "  print(\"\\nCross validation mean f1 for {}:\".format(model_name), round(mean_f1*100, 2),\"%\")\n",
    "  print(\"\\n-------------------------------------------------------------------------\\n\")\n",
    "\n",
    "  return cv_score, mean_f1\n",
    "\n",
    "# def conf_matrix(model_name, features, y_pred, y_test, cmap=\"magma\"):\n",
    "\n",
    "#   '''This function takes the predicted and test labels, generates the confusion matrix\n",
    "#   and displays it\n",
    "#   '''\n",
    "\n",
    "#   n_features = len(features)\n",
    "\n",
    "#   confussion_matrix = confusion_matrix(y_pred, y_test)\n",
    "\n",
    "#   outcomes = [\"Fake\",\"Real\"]\n",
    "#   ticks = np.arange(len(outcomes))\n",
    "\n",
    "#   fig, ax = plt.subplots()\n",
    "#   plt.xticks(ticks, outcomes)\n",
    "#   plt.yticks(ticks, outcomes)\n",
    "#   sns.heatmap(pd.DataFrame(confussion_matrix), annot=True, cmap=cmap, fmt=\"g\", xticklabels=outcomes, yticklabels=outcomes)\n",
    "#   ax.xaxis.set_label_position(\"top\")\n",
    "#   plt.tight_layout()\n",
    "#   if n_features == 1:\n",
    "\n",
    "#     plt.title(\"{} Confusion Matrix: {}\".format(model_name, features[0]), y = 1.1)\n",
    "\n",
    "#   elif n_features == 2:\n",
    "\n",
    "#     plt.title(\"{} Confusion Matrix: {} and {}\".format(model_name, features[0],features[1]), y = 1.1)\n",
    "\n",
    "#   elif n_features == 3:\n",
    "\n",
    "#     plt.title(\"{} Confusion Matrix: {}, {} and {}\".format(model_name, features[0],features[1], features[2]), y = 1.1)\n",
    "\n",
    "#   plt.ylabel(\"Actual label\")\n",
    "#   plt.xlabel(\"Predicted label\")\n",
    "\n",
    "#   return confussion_matrix\n",
    "\n",
    "def compare_models_train_test_split(models, X, y, X_train, y_train, X_test, y_test, features):\n",
    "\n",
    "  '''this fx trains the data on the four types of models,\n",
    "  generates a report with the overall accuracy of the model, the cross validation evaluation\n",
    "  and prints the confussion matrix of each model on the particular test_train split.\n",
    "\n",
    "  It returns a dictionary with the name of the model, the features that were passed during the fit,\n",
    "  the model used, the accuracy score, the classification report, the cross-validation evaluation and the confusion matrix\n",
    "  '''\n",
    "\n",
    "  model_list_dicts =[]\n",
    "\n",
    "  for model in models:\n",
    "    \n",
    "    model_name = str(model)[:-2]\n",
    "\n",
    "    if model_name == \"SVC(kernel=\\'linear\":\n",
    "\n",
    "      model_name = \"SVC\"\n",
    "    \n",
    "    model_dict = {}\n",
    "    \n",
    "    if len(features) == 14:\n",
    "      ft_str = \"rdfc\"\n",
    "      \n",
    "    elif len(features) == 8:\n",
    "      ft_str = \"corr\"\n",
    "    \n",
    "    elif len(features) == 28:\n",
    "      ft_str = \"kolmogorov_features\"\n",
    "      \n",
    "    else: \n",
    "      ft_str = \"all_features\"\n",
    "    \n",
    "    model_dict[\"name\"] = ft_str + \"__\" + model_name\n",
    "    \n",
    "    model_dict[\"features_set\"] = ft_str\n",
    "    \n",
    "    model_dict[\"n_features\"] = len(features)\n",
    "\n",
    "    model_dict[\"model\"] = model_name\n",
    "    \n",
    "    print(\"\\n\",model_name, \"\\n\", \"\\n\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    model_dict[\"accuracy\"] = accuracy\n",
    "    \n",
    "    model_dict[\"f1\"] = f1_score(y_test, y_pred)\n",
    "    \n",
    "    model_dict[\"precision\"] = precision_score(y_test, y_pred)\n",
    "    \n",
    "    model_dict[\"recall\"] = recall_score(y_test, y_pred)\n",
    "     \n",
    "     \n",
    "    print(\"OVERALL F1\", model_name, \":\", round(accuracy*100, 2),\"%\"\"\\n\")\n",
    "    \n",
    "    scores = class_report(y_test, y_pred)\n",
    "    \n",
    "    P_R_F1_true = precision_recall_fscore_support(y_test, y_pred, labels=[1])\n",
    "    precision_true, recall_true , f1_score_true  = P_R_F1_true[0], P_R_F1_true[1] ,  P_R_F1_true[2]\n",
    "    \n",
    "    P_R_F1_false = precision_recall_fscore_support(y_test, y_pred, labels=[0])\n",
    "    precision_false, recall_false, f1_score_false = P_R_F1_false[0], P_R_F1_false[1], P_R_F1_false[2]\n",
    "    \n",
    "    model_dict[\"precision_true\"], model_dict[\"recall_true\"], model_dict[\"f1_score_true\"] = precision_true, recall_true, f1_score_true\n",
    "    model_dict[\"precision_false\"], model_dict[\"recall_false\"], model_dict[\"f1_score_false\"] = precision_false, recall_false, f1_score_false\n",
    "    \n",
    "    cv_score, mean_f1 = compare_models_cross_val(model, X, y)\n",
    "    \n",
    "    model_dict[\"cv_scores\"] = cv_score\n",
    "\n",
    "    model_dict[\"mean_f1\"] = mean_f1\n",
    "    \n",
    "    # confussion_matrix = conf_matrix(model_name, features, y_pred, y_test, cmap=\"magma\")\n",
    "    \n",
    "    # model_dict[\"confussion_matrix\"] = confussion_matrix\n",
    "    \n",
    "    model_dict[\"development\"] = \"baseline\"\n",
    "\n",
    "    model_list_dicts.append(model_dict)\n",
    "    \n",
    "    pickle.dump(model, open(r\"Baseline_Models\\{}.sav\".format(ft_str + \"__\" + model_name), 'wb'))\n",
    "    \n",
    "      \n",
    "    with open(r\"Baseline_Models_Performance\\{}.json\".format(model_dict[\"name\"]), \"w\") as f:\n",
    "      json.dump(model_dict, f,  cls=NumpyArrayEncoder)\n",
    "  \n",
    "  \n",
    "  return model_list_dicts\n",
    "\n",
    "def magic(data, features):\n",
    "\n",
    "  '''this function takes all the previous functions and integrates them into a single function to run the data processing, training and testing in all the models.\n",
    "\n",
    "  It returns a dictionary with the name of the model, the features that were passed during the fit,\n",
    "  the model used, the accuracy score, the classification report, the crossvalidation evaluation and the confusion matrix\n",
    "\n",
    "  '''\n",
    "\n",
    "  data, outcomes = data_preprocess(data) # data in the correct format for the y outcomes\n",
    "\n",
    "  X, y, X_train, X_test, y_train, y_test, features = test_train(data, features) # data split for all the models with the desired features\n",
    "\n",
    "  model_list_dicts = compare_models_train_test_split(models, X, y, X_train, y_train, X_test, y_test, features) # for each model it will compute\n",
    "\n",
    "  return model_list_dicts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- array([['DESPC', '0.21438229320956675'],\n",
    "       ['DESPLw', '0.10208405348019169'],\n",
    "       ['WORD_SET_INCIDENCE_C4_COMMON_WORDS', '0.09965943628298403'],\n",
    "       ['DESWC', '0.07109169917686431'],\n",
    "       ['DESSL', '0.05266669162099124'],\n",
    "       ['TOKEN_ATTRIBUTE_RATIO_ALHPA', '0.044887257624143466'],\n",
    "       ['DESPL', '0.04316248532779844'],\n",
    "       ['DESSLd', '0.02820966410727794'],\n",
    "       ['SYNNP', '0.027405633313671348'],\n",
    "       ['WORD_PROPERTY_WRDVERB', '0.0261010048275167'],\n",
    "       ['WORD_SET_INCIDENCE_WRDPRP3p', '0.025068322443046843'],\n",
    "       ['WORD_PROPERTY_WRDFRQa', '0.02254653008839199'],\n",
    "       ['SYNLE', '0.02108048270836009'],\n",
    "       ['WORD_PROPERTY_WRDHYPnv', '0.019784971882906673'],\n",
    "       ['READFKGL', '0.018508796386778235'],\n",
    "       ['WORD_PROPERTY_CONCRETENESS', '0.017746184163603552'],\n",
    "       ['SYNMEDlem', '0.017576571201848724'],\n",
    "       ['WORD_SET_INCIDENCE_CNCCaus', '0.016597094842558495'],\n",
    "       ['TOKEN_ATTRIBUTE_RATIO_DIGIT', '0.016312327694308282'],\n",
    "       ['SYNMEDwrd', '0.01586403979613771'],\n",
    "       ['WORD_PROPERTY_WRDFRQc', '0.01478517755295334'],\n",
    "       ['DESPLd', '0.013884163425386713'],\n",
    "       ['DESWLltd', '0.01311359523902935'],\n",
    "       ['Positive_Sentiment', '0.01210902813506827'],\n",
    "       ['RDFRE', '0.011349995152157417'],\n",
    "       ['SYNSTRUTa', '0.011137636817407863'],\n",
    "       ['WORD_PROPERTY_WRDNOUN', '0.007141541942563552']], dtype='<U34') -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training dataset and the feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 65)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DESPC</th>\n",
       "      <th>DESSC</th>\n",
       "      <th>DESWC</th>\n",
       "      <th>DESPL</th>\n",
       "      <th>DESPLd</th>\n",
       "      <th>DESPLw</th>\n",
       "      <th>DESSL</th>\n",
       "      <th>DESSLd</th>\n",
       "      <th>DESWLsy</th>\n",
       "      <th>DESWLsyd</th>\n",
       "      <th>...</th>\n",
       "      <th>WORD_PROPERTY_AOA_MAX</th>\n",
       "      <th>WORD_PROPERTY_CONCRETENESS</th>\n",
       "      <th>WORD_PROPERTY_PREVALENCE</th>\n",
       "      <th>WORD_SET_INCIDENCE_C4_COMMON_WORDS</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Overall_Sentiment</th>\n",
       "      <th>Positive_Sentiment</th>\n",
       "      <th>Negative_Sentiment</th>\n",
       "      <th>Neutral_Sentiment</th>\n",
       "      <th>Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146154</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151143</td>\n",
       "      <td>0.682139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.563762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396186</td>\n",
       "      <td>0.393986</td>\n",
       "      <td>0.674648</td>\n",
       "      <td>0.685198</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.332143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.253846</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.201948</td>\n",
       "      <td>0.249849</td>\n",
       "      <td>0.248429</td>\n",
       "      <td>0.109532</td>\n",
       "      <td>0.116088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309322</td>\n",
       "      <td>0.612986</td>\n",
       "      <td>0.682585</td>\n",
       "      <td>0.252618</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.513112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.220999</td>\n",
       "      <td>0.191726</td>\n",
       "      <td>0.215594</td>\n",
       "      <td>0.199330</td>\n",
       "      <td>0.134906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297608</td>\n",
       "      <td>0.685996</td>\n",
       "      <td>0.654967</td>\n",
       "      <td>0.546360</td>\n",
       "      <td>Fake</td>\n",
       "      <td>-0.2944</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.397129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.240051</td>\n",
       "      <td>0.300706</td>\n",
       "      <td>0.189370</td>\n",
       "      <td>0.191279</td>\n",
       "      <td>0.119235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147246</td>\n",
       "      <td>0.814876</td>\n",
       "      <td>0.426085</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0.6486</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.497222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.296154</td>\n",
       "      <td>0.229730</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.220999</td>\n",
       "      <td>0.147614</td>\n",
       "      <td>0.110956</td>\n",
       "      <td>0.139367</td>\n",
       "      <td>0.084885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617585</td>\n",
       "      <td>0.218604</td>\n",
       "      <td>0.515320</td>\n",
       "      <td>0.550734</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.454167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.296154</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.227350</td>\n",
       "      <td>0.289808</td>\n",
       "      <td>0.164698</td>\n",
       "      <td>0.264674</td>\n",
       "      <td>0.203276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306847</td>\n",
       "      <td>0.621988</td>\n",
       "      <td>0.530017</td>\n",
       "      <td>0.516772</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.428052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.273077</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.037203</td>\n",
       "      <td>0.053345</td>\n",
       "      <td>0.386983</td>\n",
       "      <td>0.384460</td>\n",
       "      <td>0.050249</td>\n",
       "      <td>0.047174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323093</td>\n",
       "      <td>0.499129</td>\n",
       "      <td>0.691053</td>\n",
       "      <td>0.339440</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9590</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.395543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048029</td>\n",
       "      <td>0.075642</td>\n",
       "      <td>0.645812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370429</td>\n",
       "      <td>0.251662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606992</td>\n",
       "      <td>0.835890</td>\n",
       "      <td>0.640981</td>\n",
       "      <td>0.230619</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.620455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.534615</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.337426</td>\n",
       "      <td>0.153582</td>\n",
       "      <td>0.189306</td>\n",
       "      <td>0.225581</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.532790</td>\n",
       "      <td>0.651265</td>\n",
       "      <td>0.156495</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9532</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.473939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.138462</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.138442</td>\n",
       "      <td>0.140868</td>\n",
       "      <td>0.240751</td>\n",
       "      <td>0.445890</td>\n",
       "      <td>0.305984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447034</td>\n",
       "      <td>0.534365</td>\n",
       "      <td>0.574802</td>\n",
       "      <td>0.268774</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.461250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DESPC     DESSC     DESWC     DESPL    DESPLd    DESPLw     DESSL  \\\n",
       "0    0.444444  0.000000  0.146154  0.027027  0.000000  0.151143  0.682139   \n",
       "1    0.222222  0.176471  0.253846  0.148649  0.176471  0.201948  0.249849   \n",
       "2    0.222222  0.235294  0.276923  0.189189  0.235294  0.220999  0.191726   \n",
       "3    0.222222  0.176471  0.307692  0.148649  0.176471  0.240051  0.300706   \n",
       "4    0.222222  0.294118  0.296154  0.229730  0.294118  0.220999  0.147614   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "445  0.222222  0.176471  0.296154  0.148649  0.176471  0.227350  0.289808   \n",
       "446  0.888889  0.117647  0.273077  0.010811  0.037203  0.053345  0.386983   \n",
       "447  0.444444  0.000000  0.130769  0.000000  0.048029  0.075642  0.645812   \n",
       "448  0.222222  0.470588  0.534615  0.351351  0.352941  0.337426  0.153582   \n",
       "449  0.222222  0.176471  0.138462  0.148649  0.058824  0.138442  0.140868   \n",
       "\n",
       "       DESSLd   DESWLsy  DESWLsyd  ...  WORD_PROPERTY_AOA_MAX  \\\n",
       "0    0.000000  1.000000  0.563762  ...               0.396186   \n",
       "1    0.248429  0.109532  0.116088  ...               0.309322   \n",
       "2    0.215594  0.199330  0.134906  ...               0.297608   \n",
       "3    0.189370  0.191279  0.119235  ...               0.147246   \n",
       "4    0.110956  0.139367  0.084885  ...               0.617585   \n",
       "..        ...       ...       ...  ...                    ...   \n",
       "445  0.164698  0.264674  0.203276  ...               0.306847   \n",
       "446  0.384460  0.050249  0.047174  ...               0.323093   \n",
       "447  0.000000  0.370429  0.251662  ...               0.606992   \n",
       "448  0.189306  0.225581  0.206593  ...               0.423729   \n",
       "449  0.240751  0.445890  0.305984  ...               0.447034   \n",
       "\n",
       "     WORD_PROPERTY_CONCRETENESS  WORD_PROPERTY_PREVALENCE  \\\n",
       "0                      0.393986                  0.674648   \n",
       "1                      0.612986                  0.682585   \n",
       "2                      0.685996                  0.654967   \n",
       "3                      0.814876                  0.426085   \n",
       "4                      0.218604                  0.515320   \n",
       "..                          ...                       ...   \n",
       "445                    0.621988                  0.530017   \n",
       "446                    0.499129                  0.691053   \n",
       "447                    0.835890                  0.640981   \n",
       "448                    0.532790                  0.651265   \n",
       "449                    0.534365                  0.574802   \n",
       "\n",
       "     WORD_SET_INCIDENCE_C4_COMMON_WORDS  Polarity  Overall_Sentiment  \\\n",
       "0                              0.685198      Fake             0.7783   \n",
       "1                              0.252618      Fake             0.7763   \n",
       "2                              0.546360      Fake            -0.2944   \n",
       "3                              0.367094      Fake             0.6486   \n",
       "4                              0.550734      Fake             0.9531   \n",
       "..                                  ...       ...                ...   \n",
       "445                            0.516772      True             0.6705   \n",
       "446                            0.339440      True             0.9590   \n",
       "447                            0.230619      True             0.7964   \n",
       "448                            0.156495      True             0.9532   \n",
       "449                            0.268774      True             0.8225   \n",
       "\n",
       "     Positive_Sentiment  Negative_Sentiment  Neutral_Sentiment  Subjectivity  \n",
       "0                 0.033               0.109              0.858      0.332143  \n",
       "1                 0.045               0.106              0.849      0.513112  \n",
       "2                 0.150               0.123              0.726      0.397129  \n",
       "3                 0.000               0.032              0.968      0.497222  \n",
       "4                 0.000               0.145              0.855      0.454167  \n",
       "..                  ...                 ...                ...           ...  \n",
       "445               0.017               0.069              0.914      0.428052  \n",
       "446               0.024               0.180              0.796      0.395543  \n",
       "447               0.000               0.090              0.910      0.620455  \n",
       "448               0.089               0.175              0.736      0.473939  \n",
       "449               0.000               0.106              0.894      0.461250  \n",
       "\n",
       "[450 rows x 65 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reloading the transformed data we saved early on\n",
    "\n",
    "transformed_data = pd.read_csv(r\"Datasets\\Celebrity Dataset\\Celebrity_dataset_transformed.csv\")\n",
    "# transformed_data = transformed_data.drop(labels=\"Unnamed: 0\", axis=1)\n",
    "print(transformed_data.shape)\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(r\"Feature_Sets\\features.json\", \"r\") as f:\n",
    "    feature_sets = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corr': ['DESPC',\n",
       "  'DESPL',\n",
       "  'DESPLw',\n",
       "  'DESSL',\n",
       "  'DESSLd',\n",
       "  'READFKGL',\n",
       "  'TOKEN_ATTRIBUTE_RATIO_ALHPA',\n",
       "  'WORD_SET_INCIDENCE_C4_COMMON_WORDS'],\n",
       " 'rfecv': ['DESPC',\n",
       "  'DESWC',\n",
       "  'DESPL',\n",
       "  'DESPLd',\n",
       "  'DESPLw',\n",
       "  'DESSL',\n",
       "  'DESSLd',\n",
       "  'DESWLlt',\n",
       "  'SYNNP',\n",
       "  'SYNMEDlem',\n",
       "  'READFKGL',\n",
       "  'TOKEN_ATTRIBUTE_RATIO_ALHPA',\n",
       "  'TOKEN_ATTRIBUTE_RATIO_PUNCT',\n",
       "  'WORD_SET_INCIDENCE_C4_COMMON_WORDS'],\n",
       " 'kolmogorov': ['DESPC',\n",
       "  'DESWC',\n",
       "  'DESPL',\n",
       "  'DESPLd',\n",
       "  'DESPLw',\n",
       "  'DESSL',\n",
       "  'DESSLd',\n",
       "  'DESWLltd',\n",
       "  'SYNLE',\n",
       "  'SYNNP',\n",
       "  'SYNMEDpos',\n",
       "  'SYNMEDwrd',\n",
       "  'SYNMEDlem',\n",
       "  'SYNSTRUTa',\n",
       "  'RDFRE',\n",
       "  'READFKGL',\n",
       "  'TOKEN_ATTRIBUTE_RATIO_ALHPA',\n",
       "  'TOKEN_ATTRIBUTE_RATIO_DIGIT',\n",
       "  'WORD_SET_INCIDENCE_WRDPRP3p',\n",
       "  'WORD_SET_INCIDENCE_CNCCaus',\n",
       "  'WORD_PROPERTY_WRDNOUN',\n",
       "  'WORD_PROPERTY_WRDVERB',\n",
       "  'WORD_PROPERTY_WRDFRQc',\n",
       "  'WORD_PROPERTY_WRDFRQa',\n",
       "  'WORD_PROPERTY_WRDHYPnv',\n",
       "  'WORD_PROPERTY_CONCRETENESS',\n",
       "  'WORD_SET_INCIDENCE_C4_COMMON_WORDS',\n",
       "  'Positive_Sentiment'],\n",
       " 'all_features': ['DESPC',\n",
       "  'DESSC',\n",
       "  'DESWC',\n",
       "  'DESPL',\n",
       "  'DESPLd',\n",
       "  'DESPLw',\n",
       "  'DESSL',\n",
       "  'DESSLd',\n",
       "  'DESWLsy',\n",
       "  'DESWLsyd',\n",
       "  'DESWLlt',\n",
       "  'DESWLltd',\n",
       "  'LDTTRc',\n",
       "  'LDTTRa',\n",
       "  'LDMTLD',\n",
       "  'LDHDD',\n",
       "  'SYNLE',\n",
       "  'SYNNP',\n",
       "  'SYNMEDpos',\n",
       "  'SYNMEDwrd',\n",
       "  'SYNMEDlem',\n",
       "  'SYNSTRUTa',\n",
       "  'RDFRE',\n",
       "  'READFKGL',\n",
       "  'TOKEN_ATTRIBUTE_RATIO_ALHPA',\n",
       "  'TOKEN_ATTRIBUTE_RATIO_DIGIT',\n",
       "  'TOKEN_ATTRIBUTE_RATIO_PUNCT',\n",
       "  'TOKEN_ATTRIBUTE_RATIO_URL',\n",
       "  'WORD_SET_INCIDENCE_WRDPRP1s',\n",
       "  'WORD_SET_INCIDENCE_WRDPRP1p',\n",
       "  'WORD_SET_INCIDENCE_WRDPRP2',\n",
       "  'WORD_SET_INCIDENCE_WRDPRP3s',\n",
       "  'WORD_SET_INCIDENCE_WRDPRP3p',\n",
       "  'WORD_SET_INCIDENCE_CNCCaus',\n",
       "  'WORD_SET_INCIDENCE_CNCLogic',\n",
       "  'WORD_SET_INCIDENCE_CNCTemp',\n",
       "  'WORD_SET_INCIDENCE_CNCAdd',\n",
       "  'WORD_SET_INCIDENCE_CNCPos',\n",
       "  'WORD_SET_INCIDENCE_CNCNeg',\n",
       "  'WORD_PROPERTY_WRDNOUN',\n",
       "  'WORD_PROPERTY_WRDVERB',\n",
       "  'WORD_PROPERTY_WRDADJ',\n",
       "  'WORD_PROPERTY_WRDADV',\n",
       "  'WORD_PROPERTY_WRDFRQc',\n",
       "  'WORD_PROPERTY_WRDFRQa',\n",
       "  'WORD_PROPERTY_WRDFRQmc',\n",
       "  'WORD_PROPERTY_WRDFAMc',\n",
       "  'WORD_PROPERTY_WRDCNCc',\n",
       "  'WORD_PROPERTY_WRDIMGc',\n",
       "  'WORD_PROPERTY_WRDMEAc',\n",
       "  'WORD_PROPERTY_WRDPOLc',\n",
       "  'WORD_PROPERTY_WRDHYPn',\n",
       "  'WORD_PROPERTY_WRDHYPv',\n",
       "  'WORD_PROPERTY_WRDHYPnv',\n",
       "  'WORD_PROPERTY_AOA',\n",
       "  'WORD_PROPERTY_AOA_MAX',\n",
       "  'WORD_PROPERTY_CONCRETENESS',\n",
       "  'WORD_PROPERTY_PREVALENCE',\n",
       "  'WORD_SET_INCIDENCE_C4_COMMON_WORDS',\n",
       "  'Polarity',\n",
       "  'Overall_Sentiment',\n",
       "  'Positive_Sentiment',\n",
       "  'Negative_Sentiment',\n",
       "  'Neutral_Sentiment',\n",
       "  'Subjectivity']}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = [value for key, value in feature_sets.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(random_state=42)\n",
    "svc = SVC(kernel =\"linear\", random_state=42)\n",
    "knn = KNeighborsClassifier()\n",
    "rf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(), SVC(kernel =\"linear\"), KNeighborsClassifier(), RandomForestClassifier(random_state=42)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-report of the creation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 8 features were passed at the fit step\n",
      ":\n",
      "DESPC\n",
      "DESPL\n",
      "DESPLw\n",
      "DESSL\n",
      "DESSLd\n",
      "READFKGL\n",
      "TOKEN_ATTRIBUTE_RATIO_ALHPA\n",
      "WORD_SET_INCIDENCE_C4_COMMON_WORDS\n",
      "\n",
      " LogisticRegression \n",
      " \n",
      "\n",
      "OVERALL F1 LogisticRegression : 73.45 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.71      0.74      0.72        53\n",
      "        Real       0.76      0.73      0.75        60\n",
      "\n",
      "    accuracy                           0.73       113\n",
      "   macro avg       0.73      0.73      0.73       113\n",
      "weighted avg       0.74      0.73      0.73       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 LogisticRegression for each of the 5 iterations:\n",
      "71.29 %\n",
      "72.34 %\n",
      "69.57 %\n",
      "75.79 %\n",
      "80.0 %\n",
      "\n",
      "Cross validation mean f1 for LogisticRegression: 73.8 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " SVC \n",
      " \n",
      "\n",
      "OVERALL F1 SVC : 75.22 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.72      0.77      0.75        53\n",
      "        Real       0.79      0.73      0.76        60\n",
      "\n",
      "    accuracy                           0.75       113\n",
      "   macro avg       0.75      0.75      0.75       113\n",
      "weighted avg       0.75      0.75      0.75       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 SVC for each of the 5 iterations:\n",
      "74.0 %\n",
      "73.68 %\n",
      "68.13 %\n",
      "70.45 %\n",
      "83.87 %\n",
      "\n",
      "Cross validation mean f1 for SVC: 74.03 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " KNeighborsClassifier \n",
      " \n",
      "\n",
      "OVERALL F1 KNeighborsClassifier : 78.76 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.82      0.70      0.76        53\n",
      "        Real       0.76      0.87      0.81        60\n",
      "\n",
      "    accuracy                           0.79       113\n",
      "   macro avg       0.79      0.78      0.78       113\n",
      "weighted avg       0.79      0.79      0.79       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 KNeighborsClassifier for each of the 5 iterations:\n",
      "82.0 %\n",
      "75.0 %\n",
      "79.57 %\n",
      "84.54 %\n",
      "79.25 %\n",
      "\n",
      "Cross validation mean f1 for KNeighborsClassifier: 80.07 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " RandomForestClassifier(random_state=4 \n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_5468\\455888499.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[\"Polarity\"] = data[\"Polarity\"].replace({\"True\":1, \"Fake\":0, \"TRUE\":1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL F1 RandomForestClassifier(random_state=4 : 78.76 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.78      0.75      0.77        53\n",
      "        Real       0.79      0.82      0.80        60\n",
      "\n",
      "    accuracy                           0.79       113\n",
      "   macro avg       0.79      0.79      0.79       113\n",
      "weighted avg       0.79      0.79      0.79       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 RandomForestClassifier(random_state=4 for each of the 5 iterations:\n",
      "81.25 %\n",
      "75.51 %\n",
      "77.78 %\n",
      "85.11 %\n",
      "92.93 %\n",
      "\n",
      "Cross validation mean f1 for RandomForestClassifier(random_state=4: 82.51 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "Info: 14 features were passed at the fit step\n",
      ":\n",
      "DESPC\n",
      "DESWC\n",
      "DESPL\n",
      "DESPLd\n",
      "DESPLw\n",
      "DESSL\n",
      "DESSLd\n",
      "DESWLlt\n",
      "SYNNP\n",
      "SYNMEDlem\n",
      "READFKGL\n",
      "TOKEN_ATTRIBUTE_RATIO_ALHPA\n",
      "TOKEN_ATTRIBUTE_RATIO_PUNCT\n",
      "WORD_SET_INCIDENCE_C4_COMMON_WORDS\n",
      "\n",
      " LogisticRegression \n",
      " \n",
      "\n",
      "OVERALL F1 LogisticRegression : 82.3 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.81      0.81      0.81        53\n",
      "        Real       0.83      0.83      0.83        60\n",
      "\n",
      "    accuracy                           0.82       113\n",
      "   macro avg       0.82      0.82      0.82       113\n",
      "weighted avg       0.82      0.82      0.82       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 LogisticRegression for each of the 5 iterations:\n",
      "80.77 %\n",
      "82.69 %\n",
      "70.33 %\n",
      "78.72 %\n",
      "88.0 %\n",
      "\n",
      "Cross validation mean f1 for LogisticRegression: 80.1 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " SVC \n",
      " \n",
      "\n",
      "OVERALL F1 SVC : 83.19 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.83      0.81      0.82        53\n",
      "        Real       0.84      0.85      0.84        60\n",
      "\n",
      "    accuracy                           0.83       113\n",
      "   macro avg       0.83      0.83      0.83       113\n",
      "weighted avg       0.83      0.83      0.83       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 SVC for each of the 5 iterations:\n",
      "84.62 %\n",
      "85.15 %\n",
      "77.27 %\n",
      "83.67 %\n",
      "88.0 %\n",
      "\n",
      "Cross validation mean f1 for SVC: 83.74 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " KNeighborsClassifier \n",
      " \n",
      "\n",
      "OVERALL F1 KNeighborsClassifier : 85.84 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.82      0.89      0.85        53\n",
      "        Real       0.89      0.83      0.86        60\n",
      "\n",
      "    accuracy                           0.86       113\n",
      "   macro avg       0.86      0.86      0.86       113\n",
      "weighted avg       0.86      0.86      0.86       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 KNeighborsClassifier for each of the 5 iterations:\n",
      "88.89 %\n",
      "84.85 %\n",
      "86.02 %\n",
      "79.61 %\n",
      "84.54 %\n",
      "\n",
      "Cross validation mean f1 for KNeighborsClassifier: 84.78 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " RandomForestClassifier(random_state=4 \n",
      " \n",
      "\n",
      "OVERALL F1 RandomForestClassifier(random_state=4 : 87.61 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.84      0.91      0.87        53\n",
      "        Real       0.91      0.85      0.88        60\n",
      "\n",
      "    accuracy                           0.88       113\n",
      "   macro avg       0.88      0.88      0.88       113\n",
      "weighted avg       0.88      0.88      0.88       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 RandomForestClassifier(random_state=4 for each of the 5 iterations:\n",
      "88.89 %\n",
      "83.17 %\n",
      "80.46 %\n",
      "84.21 %\n",
      "92.93 %\n",
      "\n",
      "Cross validation mean f1 for RandomForestClassifier(random_state=4: 85.93 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "Info: 28 features were passed at the fit step\n",
      ":\n",
      "DESPC\n",
      "DESWC\n",
      "DESPL\n",
      "DESPLd\n",
      "DESPLw\n",
      "DESSL\n",
      "DESSLd\n",
      "DESWLltd\n",
      "SYNLE\n",
      "SYNNP\n",
      "SYNMEDpos\n",
      "SYNMEDwrd\n",
      "SYNMEDlem\n",
      "SYNSTRUTa\n",
      "RDFRE\n",
      "READFKGL\n",
      "TOKEN_ATTRIBUTE_RATIO_ALHPA\n",
      "TOKEN_ATTRIBUTE_RATIO_DIGIT\n",
      "WORD_SET_INCIDENCE_WRDPRP3p\n",
      "WORD_SET_INCIDENCE_CNCCaus\n",
      "WORD_PROPERTY_WRDNOUN\n",
      "WORD_PROPERTY_WRDVERB\n",
      "WORD_PROPERTY_WRDFRQc\n",
      "WORD_PROPERTY_WRDFRQa\n",
      "WORD_PROPERTY_WRDHYPnv\n",
      "WORD_PROPERTY_CONCRETENESS\n",
      "WORD_SET_INCIDENCE_C4_COMMON_WORDS\n",
      "Positive_Sentiment\n",
      "\n",
      " LogisticRegression \n",
      " \n",
      "\n",
      "OVERALL F1 LogisticRegression : 76.99 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.75      0.77      0.76        53\n",
      "        Real       0.79      0.77      0.78        60\n",
      "\n",
      "    accuracy                           0.77       113\n",
      "   macro avg       0.77      0.77      0.77       113\n",
      "weighted avg       0.77      0.77      0.77       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 LogisticRegression for each of the 5 iterations:\n",
      "78.1 %\n",
      "81.63 %\n",
      "71.26 %\n",
      "77.55 %\n",
      "84.85 %\n",
      "\n",
      "Cross validation mean f1 for LogisticRegression: 78.68 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " SVC \n",
      " \n",
      "\n",
      "OVERALL F1 SVC : 74.34 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.73      0.72      0.72        53\n",
      "        Real       0.75      0.77      0.76        60\n",
      "\n",
      "    accuracy                           0.74       113\n",
      "   macro avg       0.74      0.74      0.74       113\n",
      "weighted avg       0.74      0.74      0.74       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 SVC for each of the 5 iterations:\n",
      "80.81 %\n",
      "82.83 %\n",
      "72.53 %\n",
      "81.25 %\n",
      "85.71 %\n",
      "\n",
      "Cross validation mean f1 for SVC: 80.63 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " KNeighborsClassifier \n",
      " \n",
      "\n",
      "OVERALL F1 KNeighborsClassifier : 69.91 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.69      0.66      0.67        53\n",
      "        Real       0.71      0.73      0.72        60\n",
      "\n",
      "    accuracy                           0.70       113\n",
      "   macro avg       0.70      0.70      0.70       113\n",
      "weighted avg       0.70      0.70      0.70       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 KNeighborsClassifier for each of the 5 iterations:\n",
      "80.0 %\n",
      "69.47 %\n",
      "77.78 %\n",
      "72.0 %\n",
      "74.23 %\n",
      "\n",
      "Cross validation mean f1 for KNeighborsClassifier: 74.7 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " RandomForestClassifier(random_state=4 \n",
      " \n",
      "\n",
      "OVERALL F1 RandomForestClassifier(random_state=4 : 84.07 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.81      0.87      0.84        53\n",
      "        Real       0.88      0.82      0.84        60\n",
      "\n",
      "    accuracy                           0.84       113\n",
      "   macro avg       0.84      0.84      0.84       113\n",
      "weighted avg       0.84      0.84      0.84       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 RandomForestClassifier(random_state=4 for each of the 5 iterations:\n",
      "81.63 %\n",
      "80.0 %\n",
      "78.16 %\n",
      "84.54 %\n",
      "90.72 %\n",
      "\n",
      "Cross validation mean f1 for RandomForestClassifier(random_state=4: 83.01 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "Info: 65 features were passed at the fit step\n",
      ":\n",
      "DESPC\n",
      "DESSC\n",
      "DESWC\n",
      "DESPL\n",
      "DESPLd\n",
      "DESPLw\n",
      "DESSL\n",
      "DESSLd\n",
      "DESWLsy\n",
      "DESWLsyd\n",
      "DESWLlt\n",
      "DESWLltd\n",
      "LDTTRc\n",
      "LDTTRa\n",
      "LDMTLD\n",
      "LDHDD\n",
      "SYNLE\n",
      "SYNNP\n",
      "SYNMEDpos\n",
      "SYNMEDwrd\n",
      "SYNMEDlem\n",
      "SYNSTRUTa\n",
      "RDFRE\n",
      "READFKGL\n",
      "TOKEN_ATTRIBUTE_RATIO_ALHPA\n",
      "TOKEN_ATTRIBUTE_RATIO_DIGIT\n",
      "TOKEN_ATTRIBUTE_RATIO_PUNCT\n",
      "TOKEN_ATTRIBUTE_RATIO_URL\n",
      "WORD_SET_INCIDENCE_WRDPRP1s\n",
      "WORD_SET_INCIDENCE_WRDPRP1p\n",
      "WORD_SET_INCIDENCE_WRDPRP2\n",
      "WORD_SET_INCIDENCE_WRDPRP3s\n",
      "WORD_SET_INCIDENCE_WRDPRP3p\n",
      "WORD_SET_INCIDENCE_CNCCaus\n",
      "WORD_SET_INCIDENCE_CNCLogic\n",
      "WORD_SET_INCIDENCE_CNCTemp\n",
      "WORD_SET_INCIDENCE_CNCAdd\n",
      "WORD_SET_INCIDENCE_CNCPos\n",
      "WORD_SET_INCIDENCE_CNCNeg\n",
      "WORD_PROPERTY_WRDNOUN\n",
      "WORD_PROPERTY_WRDVERB\n",
      "WORD_PROPERTY_WRDADJ\n",
      "WORD_PROPERTY_WRDADV\n",
      "WORD_PROPERTY_WRDFRQc\n",
      "WORD_PROPERTY_WRDFRQa\n",
      "WORD_PROPERTY_WRDFRQmc\n",
      "WORD_PROPERTY_WRDFAMc\n",
      "WORD_PROPERTY_WRDCNCc\n",
      "WORD_PROPERTY_WRDIMGc\n",
      "WORD_PROPERTY_WRDMEAc\n",
      "WORD_PROPERTY_WRDPOLc\n",
      "WORD_PROPERTY_WRDHYPn\n",
      "WORD_PROPERTY_WRDHYPv\n",
      "WORD_PROPERTY_WRDHYPnv\n",
      "WORD_PROPERTY_AOA\n",
      "WORD_PROPERTY_AOA_MAX\n",
      "WORD_PROPERTY_CONCRETENESS\n",
      "WORD_PROPERTY_PREVALENCE\n",
      "WORD_SET_INCIDENCE_C4_COMMON_WORDS\n",
      "Polarity\n",
      "Overall_Sentiment\n",
      "Positive_Sentiment\n",
      "Negative_Sentiment\n",
      "Neutral_Sentiment\n",
      "Subjectivity\n",
      "\n",
      " LogisticRegression \n",
      " \n",
      "\n",
      "OVERALL F1 LogisticRegression : 100.0 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       1.00      1.00      1.00        53\n",
      "        Real       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00       113\n",
      "   macro avg       1.00      1.00      1.00       113\n",
      "weighted avg       1.00      1.00      1.00       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 LogisticRegression for each of the 5 iterations:\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "\n",
      "Cross validation mean f1 for LogisticRegression: 100.0 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " SVC \n",
      " \n",
      "\n",
      "OVERALL F1 SVC : 100.0 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       1.00      1.00      1.00        53\n",
      "        Real       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00       113\n",
      "   macro avg       1.00      1.00      1.00       113\n",
      "weighted avg       1.00      1.00      1.00       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 SVC for each of the 5 iterations:\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "\n",
      "Cross validation mean f1 for SVC: 100.0 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " KNeighborsClassifier \n",
      " \n",
      "\n",
      "OVERALL F1 KNeighborsClassifier : 100.0 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       1.00      1.00      1.00        53\n",
      "        Real       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00       113\n",
      "   macro avg       1.00      1.00      1.00       113\n",
      "weighted avg       1.00      1.00      1.00       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 KNeighborsClassifier for each of the 5 iterations:\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "\n",
      "Cross validation mean f1 for KNeighborsClassifier: 100.0 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " RandomForestClassifier(random_state=4 \n",
      " \n",
      "\n",
      "OVERALL F1 RandomForestClassifier(random_state=4 : 100.0 %\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       1.00      1.00      1.00        53\n",
      "        Real       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00       113\n",
      "   macro avg       1.00      1.00      1.00       113\n",
      "weighted avg       1.00      1.00      1.00       113\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "Cross validation f1 RandomForestClassifier(random_state=4 for each of the 5 iterations:\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "100.0 %\n",
      "\n",
      "Cross validation mean f1 for RandomForestClassifier(random_state=4: 100.0 %\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in list_features:\n",
    "    magic(transformed_data, feature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- All features input -->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
